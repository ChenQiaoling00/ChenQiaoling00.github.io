---
permalink: /
title: "Qiaoling Chen"
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Hi, this is Qiaoling Chen (Èôà Â∑ßÁé≤). I am a first-year Ph.D student majoring in Computer Science at S-Lab Nanyang Technological University (NTU), supervised by Prof. Tianwei Zhang. My research interest focuses on Machine Learning (LLM) systems. Before that, I graduated from the National University of Singapore in 2023. I was a research intern at Shanghai AI Lab, supervised by Dr. Peng Sun.


# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IWQoS 2024</div><img src='images/LinS.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[AMSP: Reducing Communication Overhead of ZeRO for Efficient LLM Training](https://arxiv.org/abs/2311.00257)

**Qiaoling Chen**, Qinghao Hu, Guoteng Wang, Yingtong Xiong, Ting Huang, Xun Chen, Yang Gao, Hang Yan, Yonggang Wen, Tianwei Zhang, Peng Sun

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint 2025</div><img src='images/SPPO.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SPPO: Efficient Long-sequence LLM Training via Adaptive Sequence Pipeline Parallel Offloading](https://arxiv.org/abs/2503.10377)

**Qiaoling Chen**, Shenggui Li, Wei Gao, Peng Sun, Yonggang Wen, Tianwei Zhang

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint 2024</div><img src='images/Buff.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[InternEvo: Efficient Long-sequence Large Language Model Training via Hybrid Parallelism and Redundant Sharding](https://arxiv.org/abs/2401.09149)

**Qiaoling Chen**, Diandian Gu, Guoteng Wang, Xun Chen, YingTong Xiong, Ting Huang, Qinghao Hu, Xin Jin, Yonggang Wen, Tianwei Zhang, Peng Sun

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NSDI 2024</div><img src='images/NSDI.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

Characterization of Large Language Model Development in the Datacenter

Qinghao Hu*, Zhisheng Ye*, Zerui Wang*, Guoteng Wang, Meng Zhang, **Qiaoling Chen**, Peng Sun, Dahua Lin, Xiaolin Wang, Yingwei Luo, Yonggang Wen, Tianwei Zhang

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">OSDI 2023</div><img src='images/hydro.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Hydro: Surrogate-Based Hyperparameter Tuning Service in Datacenters](https://tonyhao.xyz/data/OSDI_23.pdf)

Qinghao Hu, Zhisheng Ye, Meng Zhang, **Qiaoling Chen**, Peng Sun, Yonggang Wen, Tianwei Zhang

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint 2024</div><img src='images/loongtrain.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism](https://arxiv.org/pdf/2406.18485)

Diandian Gu, Peng Sun, Qinghao Hu, Ting Huang, Xun Chen, Yingtong Xiong, Guoteng Wang, **Qiaoling Chen**, Shangchun Zhao, Jiarui Fang, Yonggang Wen, Tianwei Zhang, Xin Jin, Xuanzhe Liu

</div>
</div>



# üéñ Professional Services
- *Eurosys'24*: Shadow Committee Member
- *OSDI'23*: AE Committee Member
- *ATC'23*: AE Committee Member

